seed: 42
debug: false

input_dir: ../datasets/lmsys-mix-v205
full_fit: false
save_model: false
use_wandb: false
pre_train: false
use_99: true

model:
  backbone_path: RLHFlow/pair-preference-model-LLaMA3-8B
  max_length: 1792
  num_labels: 3
  num_proc: 4
  use_gradient_checkpointing: false # true
  compile_model: false

  tokenizer:
    padding_side: left
    truncation_side: left
    use_fast: true
    max_char: 3600

  use_bnb: true
  lora:
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - up_proj
      - down_proj
    r: 32 # 32
    lora_alpha: 64
    lora_dropout: 0.01
    use_dora: false
    modules_to_save: 
      - classification_head

train_params:
  per_device_train_batch_size: 1 # 512 # 512
  per_device_eval_batch_size: 4
  num_train_epochs: 1 # 16
  gradient_accumulation_steps: 4

  warmup_pct: 0.02
  eval_frequency: 300
  patience: 20

optimizer:
  name: AdamW8bit
  head_lr: 8e-7
  lr: 8e-6
  weight_decay: 1e-3
  max_grad_norm: 64.0 # 8.0

outputs:
  model_dir: ../models/r_llama_205

wandb:
  project: lmsys
  run_name: exp001-r-llama-lora-205-pm
  tags:
    - llama
