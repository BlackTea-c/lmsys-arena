seed: 42
debug: false
fold: 0

input_dir: ../datasets/lmsys-mix-v110
full_fit: false
save_model: false
use_wandb: false
pretrain: false
use_99: true
model:
  backbone_path: google/gemma-2-9b-it
  max_length: 1600
  num_labels: 3
  num_proc: 4
  use_gradient_checkpointing: true
  
  tokenizer:
    padding_side: left
    truncation_side: left
    use_fast: true
    max_char: 3600

  use_bnb: true
  lora:
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - up_proj
      - down_proj
      # - gate_proj
    r: 16
    lora_alpha: 32
    lora_dropout: 0.01
    use_dora: false # false
    modules_to_save: 
      - classification_head

train_params:
  per_device_train_batch_size: 1 # 512 # 512
  per_device_eval_batch_size: 8
  num_train_epochs: 1 # 16
  gradient_accumulation_steps: 4

  warmup_pct: 0.02
  eval_frequency: 300
  patience: 20

optimizer:
  name: AdamW8bit
  head_lr: 1e-6
  lr: 8e-6
  weight_decay: 1e-3
  max_grad_norm: 64.0 # 8.0

outputs:
  model_dir: ../models/r_gemma_m110

wandb:
  project: lmsys
  run_name: exp001-r-gemma2-9b-m110
  tags:
    - gemma
